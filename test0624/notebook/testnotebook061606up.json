{
	"name": "testnotebook061606up",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"metadata": {
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "scala"
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"## Dim code build\n",
					"\n",
					"basic steps\n",
					"\n",
					"new_data = stagedata in dim_layout (sk with default value 0)\n",
					"\n",
					"stg_merged = new_data + current_stg (full join -> assign from right, whenright records not found in left\n",
					"assign from left, when all left records   )\n",
					"\n",
					"max_sk = max(current_stg)\n",
					"\n",
					"assign_sk = iterate stg_merged(sk = 0) and sk + max_Sk + iteration counter\n",
					"\n",
					"final_merged = assign_sk + stg_merged(sk <> 0)\n",
					"\n",
					"write back final_merged into dim_file\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"storage_location  = 'abfss://red@rsgue2fidodsta01.dfs.core.windows.net'\n",
					"stg_dir     = '/stage/master_recon/'\n",
					"stg_name    =  'parsed_central_log_partition_by_year_month_sample_ids'"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"base_path = storage_location + stg_dir\n",
					"stg_path  = base_path + stg_name\n",
					"dm_path   = storage_location + '/dm/'\n",
					"dim_path  = dm_path + 'dim_master_recon_application'\n",
					""
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"Dim function\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import *\n",
					"from pyspark.sql import functions as F\n",
					"import pandas as pd\n",
					"\n",
					"class dim_master_recon_application(object):\n",
					"\n",
					"    def layout(self):\n",
					"        return StructType([      \n",
					"                StructField('application_sk', IntegerType(), True),\n",
					"                StructField('application_name', StringType(), True),\n",
					"                StructField('application_method', StringType(), True),\n",
					"                StructField('application_service', StringType(), True),\n",
					"                StructField('esp_log_listner', StringType(), True)          \n",
					"        ])\n",
					"\n",
					"\n",
					"    def file_read(self):\n",
					"        try:\n",
					"          df_base = spark.read.format(\"delta\").load(dim_path)   \n",
					"        except:    \n",
					"          df_base = spark.createDataFrame(spark.sparkContext.parallelize([(-1, 'UA','UA','UA','UA'), (-2,'NA','NA','NA','NA')])\n",
					"                                       ,StructType((self.layout())))\n",
					"        return df_base\n",
					"        \n",
					"    def add(self,in_recs):\n",
					"     \n",
					"        df_base = self.file_read()\n",
					"\n",
					"        max_id  = df_base.agg({\"application_sk\": \"max\"}).collect()[0][\"max(application_sk)\"]\n",
					"        if max_id is None:\n",
					"                max_id = 1\n",
					"        else:\n",
					"                max_id = max_id+1\n",
					"\n",
					"        df_base.registerTempTable('df_dim_base')\n",
					"        in_recs.registerTempTable('df_dim_new')\n",
					"\n",
					"        df_merged = spark.sql(\"\"\" SELECT * FROM (SELECT CASE\n",
					"                WHEN base.application_name != '' THEN base.application_sk\n",
					"                WHEN new.app_name != '' THEN new.application_sk\n",
					"                ELSE new.application_sk\n",
					"            END AS application_sk,\n",
					"            \n",
					"            CASE\n",
					"                WHEN base.application_name != '' THEN base.application_name\n",
					"                WHEN new.app_name != '' THEN new.app_name                    \n",
					"                ELSE new.app_name\n",
					"            END AS application_name,  \n",
					"\n",
					"            CASE\n",
					"                WHEN base.application_method != '' THEN base.application_method\n",
					"                WHEN new.app_method != '' THEN new.app_method                    \n",
					"                ELSE new.app_method\n",
					"            END AS application_method, \n",
					"\n",
					"            CASE\n",
					"                WHEN base.application_service != '' THEN base.application_service\n",
					"                WHEN new.app_service != '' THEN new.app_service                    \n",
					"                ELSE new.app_service\n",
					"            END AS application_service, \n",
					"\n",
					"            CASE\n",
					"                WHEN base.esp_log_listner != '' THEN base.esp_log_listner\n",
					"                WHEN new.custom_fields_LogListener != '' THEN new.custom_fields_LogListener                    \n",
					"                ELSE new.custom_fields_LogListener\n",
					"            END AS esp_log_listner \n",
					"\n",
					"        FROM df_dim_base base\n",
					"        FULL OUTER JOIN \n",
					"        df_dim_new new\n",
					"            ON base.application_name  = new.app_name \n",
					"            and base.application_method  = new.app_method \n",
					"            and base.application_service  = new.app_service \n",
					"            and base.esp_log_listner  = new.custom_fields_LogListener\n",
					"    ) as main WHERE application_sk is not NULL order by application_sk\n",
					"\"\"\")\n",
					"\n",
					"        df_merged_sk_zero = df_merged.where(\"application_sk = 0\")\n",
					"        df_merged_sk_filled = df_merged_sk_zero.withColumn('application_sk', F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) + max_id)\n",
					"    \n",
					"        df_merged_final = pd.concat([df_merged.where(\"application_sk != 0\").toPandas(), \n",
					"                                 df_merged_sk_filled.toPandas()]) \n",
					"    #display(df_merged_final)\n",
					"        df_sdf = spark.createDataFrame(df_merged_final)\n",
					"\n",
					"        return df_sdf.write.format(\"delta\").mode(\"overwrite\").save(dim_path)\n",
					"        \n",
					"\n",
					""
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"Load function\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Read Stage File\n",
					"from pyspark.sql.functions import lit\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql import Window\n",
					"from pyspark.sql.functions import lit, col, size\n",
					"\n",
					"def load_dim_master_recon_application():\n",
					"    dm_inst = dim_master_recon_application()\n",
					"    lay_dim = dm_inst.layout()\n",
					"\n",
					"    df_stg_orig = spark.read.format(\"delta\").load(stg_path)   \n",
					"    df_stg_fltr_null = df_stg_orig.where(df_stg_orig.app_name.isNotNull())\n",
					"    df_stg = df_stg_fltr_null.where(df_stg_fltr_null.app_name != '') \n",
					"    df_selected = df_stg.select(\"system_name\",\"app_name\",\"app_method\",\"app_service\",\"custom_fields_LogListener\",\"custom_fields_method\")\n",
					"\n",
					"    df_selected.registerTempTable('df_selected_sql')\n",
					"\n",
					"    df_upd = spark.sql(\"\"\" select app_name,\n",
					"                                     case \n",
					"                                          when system_name = 'esp' and app_method like '%&flrec_%' and coalesce(custom_fields_method,'') != '' then custom_fields_method \n",
					"                                          when coalesce(app_method,'') = '' then coalesce(app_method,'NA') \n",
					"                                          else coalesce(app_method,'NA')  \n",
					"                                          end as app_method,\n",
					"                                     coalesce(app_service,'NA') as app_service,\n",
					"                                     coalesce(custom_fields_LogListener,'NA') as custom_fields_LogListener \n",
					"                                     \n",
					"                                     from df_selected_sql\n",
					"                                     \n",
					"        \"\"\")\n",
					"    \n",
					"    for col in df_upd.columns:\n",
					"      df_uniques = df_upd.withColumn(col, F.ltrim(F.rtrim(df_upd[col]))).dropDuplicates()\n",
					"    \n",
					"    df_new = df_uniques.withColumn(\"application_sk\",lit(0))\n",
					"    df_in_recs = df_new.select(\"application_sk\",\"app_name\",\"app_method\",\"app_service\",\"custom_fields_LogListener\")\n",
					"    types = [f.dataType for f in df_in_recs.schema.fields]\n",
					"    return dm_inst.add(df_in_recs)#dim_master_recon_application(df_in_recs)\n",
					""
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"load_dim_master_recon_application()\n",
					""
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df_read = spark.read.format(\"delta\").load(dim_path) \n",
					"cols = [\"application_name\", \"application_method\", \"application_service\",\"esp_log_listner\"]\n",
					"df_ord = df_read.orderBy(cols, ascending=False)\n",
					"#df_ord.show(df_read.count(),False)\n",
					"df_read.count()\n",
					"\n",
					""
				],
				"attachments": null,
				"execution_count": null
			}
		]
	}
}