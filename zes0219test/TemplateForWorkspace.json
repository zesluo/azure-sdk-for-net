{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "zes0219test"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"testlinkedservice_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'testlinkedservice'"
		},
		"zes0219test-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'zes0219test-WorkspaceDefaultSqlServer'"
		},
		"zes0219test-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://chayangstoragewestus2.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/testpipeline4')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "jhkjh uyiuyj",
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					}
				],
				"annotations": [
					"bnmbnm"
				],
				"lastPublishTime": "2021-06-08T06:25:53Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testpipeline5')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "jhkjh uyiuyj",
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					}
				],
				"annotations": [
					"bnmbnm"
				],
				"lastPublishTime": "2021-06-08T06:26:17Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testpipeline6')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "jhkjh uyiuyj",
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					}
				],
				"annotations": [
					"bnmbnm"
				],
				"lastPublishTime": "2021-06-08T06:26:42Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/zes0609testpipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "ljkh,lbvn vb",
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					}
				],
				"annotations": [
					"ag uiolui"
				],
				"lastPublishTime": "2021-06-09T04:56:34Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/zesPipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "jhkjh uyiuyj",
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					}
				],
				"annotations": [
					"bnmbnm"
				],
				"lastPublishTime": "2021-06-08T06:21:26Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testdataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "testdataset",
						"folderPath": "synapse/workspaces/zes0219test",
						"container": "synapsews1"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testlinkedservice')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('testlinkedservice_connectionString')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/zes0219test-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('zes0219test-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/zes0219test-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('zes0219test-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger1')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "zesPipeline",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Minute",
						"interval": 15,
						"startTime": "2021-06-08T05:56:00",
						"timeZone": "UTC-11"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/zesPipeline')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime0219selfhosted0501')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"description": "test",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime0219selfhosted0507')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime0219selfhosted0518')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testintrunzes0219testselfhosted0400')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testintrunzes0219testselfhosted0401')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testintrunzes0219testselfhosted0406')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testintrunzes0219testselfhosted0428')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/notebook_studio_export')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 0,
				"nbformat_minor": 0,
				"bigDataPool": {
					"referenceName": "beergrass0415",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/chayang-test-rg/providers/Microsoft.Synapse/workspaces/zes0219test/bigDataPools/beergrass0415",
						"name": "beergrass0415",
						"type": "Spark",
						"endpoint": "https://zes0219test.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/beergrass0415",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Dim code build\n",
							"\n",
							"basic steps\n",
							"\n",
							"new_data = stagedata in dim_layout (sk with default value 0)\n",
							"\n",
							"stg_merged = new_data + current_stg (full join -> assign from right, whenright records not found in left\n",
							"assign from left, when all left records   )\n",
							"\n",
							"max_sk = max(current_stg)\n",
							"\n",
							"assign_sk = iterate stg_merged(sk = 0) and sk + max_Sk + iteration counter\n",
							"\n",
							"final_merged = assign_sk + stg_merged(sk <> 0)\n",
							"\n",
							"write back final_merged into dim_file\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_location  = 'abfss://red@rsgue2fidodsta01.dfs.core.windows.net'\n",
							"stg_dir     = '/stage/master_recon/'\n",
							"stg_name    =  'parsed_central_log_partition_by_year_month_sample_ids'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"base_path = storage_location + stg_dir\n",
							"stg_path  = base_path + stg_name\n",
							"dm_path   = storage_location + '/dm/'\n",
							"dim_path  = dm_path + 'dim_master_recon_roxie_order_status'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Read Stage File\n",
							"from pyspark.sql.functions import lit\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"df_stg_orig = spark.read.format(\"delta\").load(stg_path)   \n",
							"\n",
							"df_stg = df_stg_orig.filter((col(\"system_name\") == 'roxie') & ((col(\"response_order_status_code\") != '') | (col(\"response_order_status_code\").isNotNull())))\n",
							"\n",
							"#df_stg_roxie = df_stg_orig.where(df_stg_fltr_null.sys = 'roxie')\n",
							"#df_stg_fltr_null = df_stg_roxie.where(df_stg_orig.response_processing_status.isNotNull())\n",
							"#df_stg = df_stg_fltr_null.where(df_stg_fltr_null.response_processing_status != '')\n",
							"# Read Base File\n",
							"try:\n",
							"    df_base = spark.read.format(\"delta\").load(dim_path)   \n",
							"except:\n",
							"    from pyspark.sql.types import *\n",
							"    df_base = spark.createDataFrame(spark.sparkContext.parallelize([(-1, 'UA','UA'), (-2, 'NA','NA')]),\n",
							"        StructType([      \n",
							"            StructField('order_status_sk', IntegerType(), True),\n",
							"            StructField('order_status_cd', StringType(), True),\n",
							"            StructField('order_status_desc', StringType(), True),\n",
							"        ])\n",
							"    )\n",
							"\n",
							"# Prep Stage File\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql import Window\n",
							"\n",
							"\n",
							"max_id     = df_base.agg({\"order_status_sk\": \"max\"}).collect()[0][\"max(order_status_sk)\"]\n",
							"if max_id is None:\n",
							"    max_id = 1\n",
							"else:\n",
							"    max_id = max_id+1\n",
							"df_uniques = df_stg.select(\"response_order_status_code\").dropDuplicates()\n",
							"# df_new     = df_uniques.toPandas().withColumn('event_code_sk', 0)\n",
							"df_new = df_uniques.withColumn(\"order_status_sk\",lit(0))\n",
							"df_uniques.show()\n",
							"df_new.show()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"order_status_cd"
									],
									"values": [
										"order_status_sk"
									],
									"yLabel": "order_status_sk",
									"xLabel": "order_status_cd",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"order_status_sk\":{\"100\":3,\"201\":1,\"402\":2,\"NA\":-2,\"UA\":-1}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"from pyspark.sql import functions as F\n",
							"import pandas as pd\n",
							"\n",
							"df_base.registerTempTable('df_dim_base')\n",
							"df_new.registerTempTable('df_dim_new')\n",
							"df_merged = spark.sql(\"\"\" SELECT * FROM (SELECT CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_sk\n",
							"                WHEN new.response_order_status_code != '' THEN new.order_status_sk\n",
							"                ELSE new.order_status_sk\n",
							"            END AS order_status_sk,\n",
							"            \n",
							"            CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_cd\n",
							"                WHEN new.response_order_status_code != '' THEN new.response_order_status_code                    \n",
							"                ELSE new.response_order_status_code\n",
							"            END AS order_status_cd,  \n",
							"\n",
							"            CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_desc\n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '100' THEN 'Successful Order'                    \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '201' THEN 'Non Billable'  \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '401' THEN 'Insufficient Data' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '402' THEN 'Invalid Acct Or Node' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '403' THEN 'Error From Vendor' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '404' THEN 'Internal Error' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '405' THEN 'Gateway Error'  \n",
							"                ELSE new.response_order_status_code\n",
							"            END AS order_status_desc\n",
							"\n",
							"        FROM df_dim_base base\n",
							"        FULL OUTER JOIN df_dim_new new\n",
							"            ON base.order_status_cd = new.response_order_status_code\n",
							"    ) as main WHERE order_status_sk is not NULL order by order_status_sk\n",
							"\"\"\")\n",
							"\n",
							"df_merged_sk_zero = df_merged.where(\"order_status_sk = 0\")\n",
							"df_merged_sk_filled = df_merged_sk_zero.withColumn('order_status_sk', F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) + max_id)\n",
							"\n",
							"# pd.concat([survey_sub, survey_sub_last10], axis=0)\n",
							"\n",
							"df_merged_final = pd.concat([df_merged.where(\"order_status_sk != 0\").toPandas(), df_merged_sk_filled.toPandas()]) \n",
							"# df_merged.where(\"event_code_sk != 0\") + df_merged_sk_filled\n",
							"# df_merged_sk_filled.show()\n",
							"display(df_merged_final)\n",
							"df_sdf = spark.createDataFrame(df_merged_final)\n",
							"\n",
							"df_sdf.write.format(\"delta\").mode(\"overwrite\").save(dim_path)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testnote052701')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"tmp=\"hi\""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testnote052801')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"tmp=\"hi\""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testnote052802')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"tmp=\"hi\""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testnote060201')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"tmp=\"hi\""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testnote060202')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"tmp=\"hi\""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testnote060202copy')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"tmp=\"hi\""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testnote060801')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Dim code build\n",
							"\n",
							"basic steps\n",
							"\n",
							"new_data = stagedata in dim_layout (sk with default value 0)\n",
							"\n",
							"stg_merged = new_data + current_stg (full join -> assign from right, whenright records not found in left\n",
							"assign from left, when all left records   )\n",
							"\n",
							"max_sk = max(current_stg)\n",
							"\n",
							"assign_sk = iterate stg_merged(sk = 0) and sk + max_Sk + iteration counter\n",
							"\n",
							"final_merged = assign_sk + stg_merged(sk <> 0)\n",
							"\n",
							"write back final_merged into dim_file\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_location  = 'abfss://red@rsgue2fidodsta01.dfs.core.windows.net'\n",
							"stg_dir     = '/stage/master_recon/'\n",
							"stg_name    =  'parsed_central_log_partition_by_year_month_sample_ids'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"base_path = storage_location + stg_dir\n",
							"stg_path  = base_path + stg_name\n",
							"dm_path   = storage_location + '/dm/'\n",
							"dim_path  = dm_path + 'dim_master_recon_roxie_order_status'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Read Stage File\n",
							"from pyspark.sql.functions import lit\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"df_stg_orig = spark.read.format(\"delta\").load(stg_path)   \n",
							"\n",
							"df_stg = df_stg_orig.filter((col(\"system_name\") == 'roxie') & ((col(\"response_order_status_code\") != '') | (col(\"response_order_status_code\").isNotNull())))\n",
							"\n",
							"#df_stg_roxie = df_stg_orig.where(df_stg_fltr_null.sys = 'roxie')\n",
							"#df_stg_fltr_null = df_stg_roxie.where(df_stg_orig.response_processing_status.isNotNull())\n",
							"#df_stg = df_stg_fltr_null.where(df_stg_fltr_null.response_processing_status != '')\n",
							"# Read Base File\n",
							"try:\n",
							"    df_base = spark.read.format(\"delta\").load(dim_path)   \n",
							"except:\n",
							"    from pyspark.sql.types import *\n",
							"    df_base = spark.createDataFrame(spark.sparkContext.parallelize([(-1, 'UA','UA'), (-2, 'NA','NA')]),\n",
							"        StructType([      \n",
							"            StructField('order_status_sk', IntegerType(), True),\n",
							"            StructField('order_status_cd', StringType(), True),\n",
							"            StructField('order_status_desc', StringType(), True),\n",
							"        ])\n",
							"    )\n",
							"\n",
							"# Prep Stage File\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql import Window\n",
							"\n",
							"\n",
							"max_id     = df_base.agg({\"order_status_sk\": \"max\"}).collect()[0][\"max(order_status_sk)\"]\n",
							"if max_id is None:\n",
							"    max_id = 1\n",
							"else:\n",
							"    max_id = max_id+1\n",
							"df_uniques = df_stg.select(\"response_order_status_code\").dropDuplicates()\n",
							"# df_new     = df_uniques.toPandas().withColumn('event_code_sk', 0)\n",
							"df_new = df_uniques.withColumn(\"order_status_sk\",lit(0))\n",
							"df_uniques.show()\n",
							"df_new.show()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"order_status_cd"
									],
									"values": [
										"order_status_sk"
									],
									"yLabel": "order_status_sk",
									"xLabel": "order_status_cd",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"order_status_sk\":{\"100\":3,\"201\":1,\"402\":2,\"NA\":-2,\"UA\":-1}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"from pyspark.sql import functions as F\n",
							"import pandas as pd\n",
							"\n",
							"df_base.registerTempTable('df_dim_base')\n",
							"df_new.registerTempTable('df_dim_new')\n",
							"df_merged = spark.sql(\"\"\" SELECT * FROM (SELECT CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_sk\n",
							"                WHEN new.response_order_status_code != '' THEN new.order_status_sk\n",
							"                ELSE new.order_status_sk\n",
							"            END AS order_status_sk,\n",
							"            \n",
							"            CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_cd\n",
							"                WHEN new.response_order_status_code != '' THEN new.response_order_status_code                    \n",
							"                ELSE new.response_order_status_code\n",
							"            END AS order_status_cd,  \n",
							"\n",
							"            CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_desc\n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '100' THEN 'Successful Order'                    \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '201' THEN 'Non Billable'  \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '401' THEN 'Insufficient Data' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '402' THEN 'Invalid Acct Or Node' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '403' THEN 'Error From Vendor' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '404' THEN 'Internal Error' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '405' THEN 'Gateway Error'  \n",
							"                ELSE new.response_order_status_code\n",
							"            END AS order_status_desc\n",
							"\n",
							"        FROM df_dim_base base\n",
							"        FULL OUTER JOIN df_dim_new new\n",
							"            ON base.order_status_cd = new.response_order_status_code\n",
							"    ) as main WHERE order_status_sk is not NULL order by order_status_sk\n",
							"\"\"\")\n",
							"\n",
							"df_merged_sk_zero = df_merged.where(\"order_status_sk = 0\")\n",
							"df_merged_sk_filled = df_merged_sk_zero.withColumn('order_status_sk', F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) + max_id)\n",
							"\n",
							"# pd.concat([survey_sub, survey_sub_last10], axis=0)\n",
							"\n",
							"df_merged_final = pd.concat([df_merged.where(\"order_status_sk != 0\").toPandas(), df_merged_sk_filled.toPandas()]) \n",
							"# df_merged.where(\"event_code_sk != 0\") + df_merged_sk_filled\n",
							"# df_merged_sk_filled.show()\n",
							"display(df_merged_final)\n",
							"df_sdf = spark.createDataFrame(df_merged_final)\n",
							"\n",
							"df_sdf.write.format(\"delta\").mode(\"overwrite\").save(dim_path)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testnote060802')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"metadata": {
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Dim code build\n",
							"\n",
							"basic steps\n",
							"\n",
							"new_data = stagedata in dim_layout (sk with default value 0)\n",
							"\n",
							"stg_merged = new_data + current_stg (full join -> assign from right, whenright records not found in left\n",
							"assign from left, when all left records   )\n",
							"\n",
							"max_sk = max(current_stg)\n",
							"\n",
							"assign_sk = iterate stg_merged(sk = 0) and sk + max_Sk + iteration counter\n",
							"\n",
							"final_merged = assign_sk + stg_merged(sk <> 0)\n",
							"\n",
							"write back final_merged into dim_file\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_location  = 'abfss://red@rsgue2fidodsta01.dfs.core.windows.net'\n",
							"stg_dir     = '/stage/master_recon/'\n",
							"stg_name    =  'parsed_central_log_partition_by_year_month_sample_ids'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"base_path = storage_location + stg_dir\n",
							"stg_path  = base_path + stg_name\n",
							"dm_path   = storage_location + '/dm/'\n",
							"dim_path  = dm_path + 'dim_master_recon_roxie_order_status'\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Read Stage File\n",
							"from pyspark.sql.functions import lit\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"df_stg_orig = spark.read.format(\"delta\").load(stg_path)   \n",
							"\n",
							"df_stg = df_stg_orig.filter((col(\"system_name\") == 'roxie') & ((col(\"response_order_status_code\") != '') | (col(\"response_order_status_code\").isNotNull())))\n",
							"\n",
							"#df_stg_roxie = df_stg_orig.where(df_stg_fltr_null.sys = 'roxie')\n",
							"#df_stg_fltr_null = df_stg_roxie.where(df_stg_orig.response_processing_status.isNotNull())\n",
							"#df_stg = df_stg_fltr_null.where(df_stg_fltr_null.response_processing_status != '')\n",
							"# Read Base File\n",
							"try:\n",
							"    df_base = spark.read.format(\"delta\").load(dim_path)   \n",
							"except:\n",
							"    from pyspark.sql.types import *\n",
							"    df_base = spark.createDataFrame(spark.sparkContext.parallelize([(-1, 'UA','UA'), (-2, 'NA','NA')]),\n",
							"        StructType([      \n",
							"            StructField('order_status_sk', IntegerType(), True),\n",
							"            StructField('order_status_cd', StringType(), True),\n",
							"            StructField('order_status_desc', StringType(), True),\n",
							"        ])\n",
							"    )\n",
							"\n",
							"# Prep Stage File\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql import Window\n",
							"\n",
							"\n",
							"max_id     = df_base.agg({\"order_status_sk\": \"max\"}).collect()[0][\"max(order_status_sk)\"]\n",
							"if max_id is None:\n",
							"    max_id = 1\n",
							"else:\n",
							"    max_id = max_id+1\n",
							"df_uniques = df_stg.select(\"response_order_status_code\").dropDuplicates()\n",
							"# df_new     = df_uniques.toPandas().withColumn('event_code_sk', 0)\n",
							"df_new = df_uniques.withColumn(\"order_status_sk\",lit(0))\n",
							"df_uniques.show()\n",
							"df_new.show()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"order_status_cd"
									],
									"values": [
										"order_status_sk"
									],
									"yLabel": "order_status_sk",
									"xLabel": "order_status_cd",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"order_status_sk\":{\"100\":3,\"201\":1,\"402\":2,\"NA\":-2,\"UA\":-1}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"from pyspark.sql import functions as F\n",
							"import pandas as pd\n",
							"\n",
							"df_base.registerTempTable('df_dim_base')\n",
							"df_new.registerTempTable('df_dim_new')\n",
							"df_merged = spark.sql(\"\"\" SELECT * FROM (SELECT CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_sk\n",
							"                WHEN new.response_order_status_code != '' THEN new.order_status_sk\n",
							"                ELSE new.order_status_sk\n",
							"            END AS order_status_sk,\n",
							"            \n",
							"            CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_cd\n",
							"                WHEN new.response_order_status_code != '' THEN new.response_order_status_code                    \n",
							"                ELSE new.response_order_status_code\n",
							"            END AS order_status_cd,  \n",
							"\n",
							"            CASE\n",
							"                WHEN base.order_status_cd != '' THEN base.order_status_desc\n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '100' THEN 'Successful Order'                    \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '201' THEN 'Non Billable'  \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '401' THEN 'Insufficient Data' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '402' THEN 'Invalid Acct Or Node' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '403' THEN 'Error From Vendor' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '404' THEN 'Internal Error' \n",
							"                WHEN new.response_order_status_code != '' and new.response_order_status_code = '405' THEN 'Gateway Error'  \n",
							"                ELSE new.response_order_status_code\n",
							"            END AS order_status_desc\n",
							"\n",
							"        FROM df_dim_base base\n",
							"        FULL OUTER JOIN df_dim_new new\n",
							"            ON base.order_status_cd = new.response_order_status_code\n",
							"    ) as main WHERE order_status_sk is not NULL order by order_status_sk\n",
							"\"\"\")\n",
							"\n",
							"df_merged_sk_zero = df_merged.where(\"order_status_sk = 0\")\n",
							"df_merged_sk_filled = df_merged_sk_zero.withColumn('order_status_sk', F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) + max_id)\n",
							"\n",
							"# pd.concat([survey_sub, survey_sub_last10], axis=0)\n",
							"\n",
							"df_merged_final = pd.concat([df_merged.where(\"order_status_sk != 0\").toPandas(), df_merged_sk_filled.toPandas()]) \n",
							"# df_merged.where(\"event_code_sk != 0\") + df_merged_sk_filled\n",
							"# df_merged_sk_filled.show()\n",
							"display(df_merged_final)\n",
							"df_sdf = spark.createDataFrame(df_merged_final)\n",
							"\n",
							"df_sdf.write.format(\"delta\").mode(\"overwrite\").save(dim_path)\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark job definition 1')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "beergrass0415",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "2.4",
				"language": "python",
				"jobProperties": {
					"name": "Spark job definition 1",
					"file": "abfss://synapsews1@chayangstoragewestus2.dfs.core.windows.net/synapse/workspaces/zes0219test/batchjobs/Spark%20job%20definition%201/wordcount.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					},
					"args": [
						"abfss://synapsews1@chayangstoragewestus2.dfs.core.windows.net/synapsews1/user/zesluo/shakespeare.txt",
						"abfss://synapsews1@chayangstoragewestus2.dfs.core.windows.net/synapsews1/user/zesluo/result0"
					],
					"jars": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/rivertiger0220')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"restorePointInTime": "0001-01-01T00:00:00",
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2euap"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook2",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "notebook_studio_export",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					}
				],
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/notebook_studio_export')]"
			]
		}
	]
}